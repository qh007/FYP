# split ocean proximity into different dummy variables
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# split ocean proximity into different dummy variables
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
x_col
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
View(housing_new)
22-1+5
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing$median_house_value - price_mean
write.table(housing_new, file = "housing_new.csv", sep = "\t", row.names = F)
write.table(housing_new, file = "housing_new.csv", sep = "", row.names = F)
library(WriteXLS)
install.packages("WriteXLS")
library(WriteXLS)
library(WriteXLS)
install.packages("WriteXLS")
library(WriteXLS)
writeWorksheetToFile("Data/housing_new.xlsx", data = housing_new, sheet = "Sheet 1",
startRow = 1, startCol = 1)
install.packages("XLConnect")
library(XLConnect)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
write.csv(df,file="Data/housing_new.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 11*10/2 = 81 columns
View(housing_new)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing$median_house_value - price_mean
write.csv(df,file="Data/housing_new.csv", row.names=FALSE)
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
View(housing)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
view(housing_new)
View(housing_new)
mean(housing_new[[stand_col[5]]]
)
mean(housing_new[5])
mean(housing_new[[5]])
mean(housing_new[['total_bedrooms']])
View(housing)
# total columns = 26 + 11*10/2 = 81 columns
View(housing_new)
housing_new = na.omit(housing_new)
# total columns = 26 + 11*10/2 = 81 columns
View(housing_new)
# remove all na values
housing_new = na.omit(housing_new)
# total columns = 26 + 11*10/2 = 81 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing$median_house_value - price_mean
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 11*10/2 = 81 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
View(housing_new)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
# set inland as base case and dont add its interactions
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# drop all interactions of ocean_proximity category
housing_new = housing_new%>% select(-c('ISLAND_NEAR_BAY', 'ISLAND_NEAR_OCEAN', 'ISLAND_X1H_OCEAN', 'NEAR_BAY_NEAR_OCEAN', 'NEAR_BAY_X1H_OCEAN', 'NEAR_OCEAN_X1H_OCEAN'))
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
# set INLAND as base case and dont add its interactions
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# drop all interactions of ocean_proximity category
housing_new = housing_new%>% select(-c('ISLAND_NEAR_BAY', 'ISLAND_NEAR_OCEAN', 'ISLAND_X1H_OCEAN', 'NEAR_BAY_NEAR_OCEAN', 'NEAR_BAY_X1H_OCEAN', 'NEAR_OCEAN_X1H_OCEAN'))
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/Problem Sets/Problem set 6')
library(AER)
library(stargazer)
library(dplyr)
library(readxl)
library(tidyr)
library(glmnet)
library(caret)
library(dplyr)
CASchools_IS = read_excel("CASchools_EE141_InSample.xlsx")
CASchools_OOS = read_excel("CASchools_EE141_OutOfSample.xlsx")
# construct new dataset to include squares and interactions for predictors
all_col = colnames(CASchools_IS)
x_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore","charter_s")]
# add squares
for (i in seq(1, length(x_col))) {CASchools_IS[paste(x_col[i],'squared', sep = '_')] = CASchools_IS[x_col[i]]^2}
# add interactions
x_col = append(x_col, "charter_s")
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
CASchools_IS[paste(x_col[i],x_col[j], sep = '_')] = CASchools_IS[x_col[i]]*CASchools_IS[x_col[j]]} }}
# check that have 234 columns (229 predictors + 5others)
length(colnames(CASchools_IS)) == 234
# summary stats for predictors before standarization
CA = CASchools_IS %>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
means <- colMeans(CA)
sds <- apply(CA, 2, sd)
# Create a new data frame
summary_pre <- data.frame(mean = means, sd = sds)
# summary stats for predictors before standarization
CA = CASchools_IS %>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
means <- colMeans(CA)
sds <- apply(CA, 2, sd)
# Create a new data frame
summary_pre <- data.frame(mean = means, sd = sds)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
CASchool_stand = CASchools_IS
all_col = colnames(CASchools_IS)
stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(stand_col))) {
CASchool_stand[stand_col[i]] = (CASchool_stand[stand_col[i]] - mean(CASchool_stand[[stand_col[i]]])) / sd(CASchool_stand[[stand_col[i]]]) }
# sample mean of TestScore
testscore_mean = mean(CASchool_stand$testscore)
testscore_mean
CASchool_stand$testscore = CASchool_stand$testscore - testscore_mean
# OLS regression results
CASchool_stand = CASchool_stand %>% select(-c("countyname","districtname","schoolname","zipcode"))
model <- lm(testscore ~. - 1, data = CASchool_stand)
summary(model)
# note that for lambda shd be 300/500
CA_x = CASchool_stand %>% select(-c(testscore))
ridge_model <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha = 0,  lambda = 300/500, intercept = FALSE, standardize = FALSE)
summary(ridge_model)
# print(coef(ridge_model))
# lambda shd be 1000/500
lasso_model  <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha =1,  lambda = 2, intercept = FALSE, standardize = FALSE)
print(lasso_model)
non_zero_counts <- apply(coef(lasso_model) != 0, 2, sum)
print(non_zero_counts)
pca_result <- prcomp(CA_x, scale. = TRUE)
variance_proportion <- pca_result$sdev^2 / sum(pca_result$sdev^2)
plot(variance_proportion, type = "b", xlab = "Principal Component", ylab = "Proportion of Variance Explained", main = "Scree Plot")
variance_proportion[1]*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:15])*100
first_15_components <- pca_result$x[, 1:15]
print(head(first_15_components))
model_pca = lm(CASchool_stand$testscore~first_15_components - 1)
summary(model_pca)
CASchools_OOS = read_excel("CASchools_EE141_OutOfSample.xlsx")
# construct new dataset to include squares and interactions for predictors
all_col = colnames(CASchools_OOS)
x_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore","charter_s")]
# add squares
for (i in seq(1, length(x_col))) {CASchools_OOS[paste(x_col[i],'squared', sep = '_')] = CASchools_OOS[x_col[i]]^2}
# add interactions
x_col = append(x_col, "charter_s")
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
CASchools_OOS[paste(x_col[i],x_col[j], sep = '_')] = CASchools_OOS[x_col[i]]*CASchools_OOS[x_col[j]]} }}
# standarise predictors
CASchools_OOS_stand = CASchools_OOS
all_col = colnames(CASchools_OOS_stand)
stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(x_col))) {
CASchools_OOS_stand[stand_col[i]] = (CASchools_OOS_stand[stand_col[i]] - mean(CASchools_OOS_stand[[stand_col[i]]])) / sd(CASchools_OOS_stand[[stand_col[i]]]) }
# demean testscore
testscore_mean = mean(CASchools_OOS_stand$testscore)
CASchools_OOS_stand$testscore = CASchools_OOS_stand$testscore - testscore_mean
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test))
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test))
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
pred_pca = predict(model_pca, newx = as.matrix(x_test))
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10)
plot(cv_ridge)
cv_ridge$lambda.min
# CV for lasso  regression
cv_lasso <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 1, nfolds = 10)
plot(cv_lasso)
cv_lasso$lambda.min
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10, type.measure = 'mse')
plot(cv_ridge)
cv_ridge$lambda.min
# CV for PCA
# Specify the range of possible numbers of components
num_components_range <- 1:100
# Create a data frame to store the results
results <- data.frame(num_components = num_components_range, RMSE = rep(NA, length(num_components_range)))
# Specify the cross-validation settings
ctrl <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Iterate over the range of possible numbers of components
for (i in num_components_range) {
# Perform PCA
pca_model <- prcomp(CA_x, scale. = TRUE)
# Extract principal components
x_train_pca <- predict(pca_model, newdata = CA_x)[, 1:i]
x_train_pca_df <- as.data.frame(x_train_pca)
colnames(x_train_pca_df) <- paste0("PC", 1:i)
# Fit a predictive model (e.g., linear regression) on the reduced-dimensional data using cross-validation
lm_model <- train(x = x_train_pca_df, y = CASchool_stand$testscore, method = "lm", trControl = ctrl)
# Store the RMSE from cross-validation
results$RMSE[i] <- min(lm_model$results$RMSE)
}
# Find the number of components with the lowest RMSE
optimal_num_components <- results$num_components[which.min(results$RMSE)]
# Print the optimal number of components
print(optimal_num_components)
# model using optimal lambdas
ridge_model <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha = 0,  lambda = cv_ridge$lambda.min, intercept = FALSE)
lasso_model  <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha =1,  lambda = cv_lasso$lambda.min, intercept = FALSE)
first_13_components <- pca_result$x[, 1:13]
model_pca = lm(CASchool_stand$testscore~first_13_components - 1)
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test), s =cv_ridge$lambda.min)
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test), s=cv_lasso$lambda.min)
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
x_test_pca = predict(pca_result, newdata = x_test)[, 1:23]
pred_pca = predict(model_pca, newx = x_test_pca)
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
install.packages("strucchange")
install.packages("strucchange")
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP')
install.packages("strucchange")
R CMD INSTALL --no-lock <strucchange>
install.packages("strucchange", dependencies=TRUE, INSTALL_opts = c('--no-lock'))
gc()
install.packages(c("AER", "aTSA", "callr", "cli", "commonmark", "curl", "data.table", "digest", "fansi", "forecast", "fracdiff", "ggplot2", "glue", "httpgd", "httpuv", "insight", "jsonlite", "mclust", "multicool", "olsrr", "pkgbuild", "plot3D", "processx", "ps", "quantmod", "R.oo", "RcppArmadillo", "RcppEigen", "remotes", "rlang", "rmarkdown", "roxygen2", "sass", "stringi", "survival", "systemfonts", "tidyr", "tidyselect", "tinytex", "utf8", "xfun", "xml2", "xts", "yaml"))
R CMD INSTALL [options] [l-lib] pkgs
