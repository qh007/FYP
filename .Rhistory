housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN", "INLAND"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 11*10/2 = 81 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
View(housing_new)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
# set inland as base case and dont add its interactions
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# drop all interactions of ocean_proximity category
housing_new = housing_new%>% select(-c('ISLAND_NEAR_BAY', 'ISLAND_NEAR_OCEAN', 'ISLAND_X1H_OCEAN', 'NEAR_BAY_NEAR_OCEAN', 'NEAR_BAY_X1H_OCEAN', 'NEAR_OCEAN_X1H_OCEAN'))
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[x_col[i]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[x_col[i]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
# set INLAND as base case and dont add its interactions
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
housing_new[paste(x_col[i],x_col[j], sep = '_')] = housing_new[x_col[i]]*housing_new[x_col[j]]} }}
# total columns = 26 + 10*9/2 = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# drop all interactions of ocean_proximity category
housing_new = housing_new%>% select(-c('ISLAND_NEAR_BAY', 'ISLAND_NEAR_OCEAN', 'ISLAND_X1H_OCEAN', 'NEAR_BAY_NEAR_OCEAN', 'NEAR_BAY_X1H_OCEAN', 'NEAR_OCEAN_X1H_OCEAN'))
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude","median_house_value")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
price_mean = mean(housing_new$median_house_value)
housing_new$median_house_value = housing_new$median_house_value - price_mean
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/Problem Sets/Problem set 6')
library(AER)
library(stargazer)
library(dplyr)
library(readxl)
library(tidyr)
library(glmnet)
library(caret)
library(dplyr)
CASchools_IS = read_excel("CASchools_EE141_InSample.xlsx")
CASchools_OOS = read_excel("CASchools_EE141_OutOfSample.xlsx")
# construct new dataset to include squares and interactions for predictors
all_col = colnames(CASchools_IS)
x_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore","charter_s")]
# add squares
for (i in seq(1, length(x_col))) {CASchools_IS[paste(x_col[i],'squared', sep = '_')] = CASchools_IS[x_col[i]]^2}
# add interactions
x_col = append(x_col, "charter_s")
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
CASchools_IS[paste(x_col[i],x_col[j], sep = '_')] = CASchools_IS[x_col[i]]*CASchools_IS[x_col[j]]} }}
# check that have 234 columns (229 predictors + 5others)
length(colnames(CASchools_IS)) == 234
# summary stats for predictors before standarization
CA = CASchools_IS %>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
means <- colMeans(CA)
sds <- apply(CA, 2, sd)
# Create a new data frame
summary_pre <- data.frame(mean = means, sd = sds)
# summary stats for predictors before standarization
CA = CASchools_IS %>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
means <- colMeans(CA)
sds <- apply(CA, 2, sd)
# Create a new data frame
summary_pre <- data.frame(mean = means, sd = sds)
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
CASchool_stand = CASchools_IS
all_col = colnames(CASchools_IS)
stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(stand_col))) {
CASchool_stand[stand_col[i]] = (CASchool_stand[stand_col[i]] - mean(CASchool_stand[[stand_col[i]]])) / sd(CASchool_stand[[stand_col[i]]]) }
# sample mean of TestScore
testscore_mean = mean(CASchool_stand$testscore)
testscore_mean
CASchool_stand$testscore = CASchool_stand$testscore - testscore_mean
# OLS regression results
CASchool_stand = CASchool_stand %>% select(-c("countyname","districtname","schoolname","zipcode"))
model <- lm(testscore ~. - 1, data = CASchool_stand)
summary(model)
# note that for lambda shd be 300/500
CA_x = CASchool_stand %>% select(-c(testscore))
ridge_model <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha = 0,  lambda = 300/500, intercept = FALSE, standardize = FALSE)
summary(ridge_model)
# print(coef(ridge_model))
# lambda shd be 1000/500
lasso_model  <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha =1,  lambda = 2, intercept = FALSE, standardize = FALSE)
print(lasso_model)
non_zero_counts <- apply(coef(lasso_model) != 0, 2, sum)
print(non_zero_counts)
pca_result <- prcomp(CA_x, scale. = TRUE)
variance_proportion <- pca_result$sdev^2 / sum(pca_result$sdev^2)
plot(variance_proportion, type = "b", xlab = "Principal Component", ylab = "Proportion of Variance Explained", main = "Scree Plot")
variance_proportion[1]*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:15])*100
first_15_components <- pca_result$x[, 1:15]
print(head(first_15_components))
model_pca = lm(CASchool_stand$testscore~first_15_components - 1)
summary(model_pca)
CASchools_OOS = read_excel("CASchools_EE141_OutOfSample.xlsx")
# construct new dataset to include squares and interactions for predictors
all_col = colnames(CASchools_OOS)
x_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore","charter_s")]
# add squares
for (i in seq(1, length(x_col))) {CASchools_OOS[paste(x_col[i],'squared', sep = '_')] = CASchools_OOS[x_col[i]]^2}
# add interactions
x_col = append(x_col, "charter_s")
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
CASchools_OOS[paste(x_col[i],x_col[j], sep = '_')] = CASchools_OOS[x_col[i]]*CASchools_OOS[x_col[j]]} }}
# standarise predictors
CASchools_OOS_stand = CASchools_OOS
all_col = colnames(CASchools_OOS_stand)
stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(x_col))) {
CASchools_OOS_stand[stand_col[i]] = (CASchools_OOS_stand[stand_col[i]] - mean(CASchools_OOS_stand[[stand_col[i]]])) / sd(CASchools_OOS_stand[[stand_col[i]]]) }
# demean testscore
testscore_mean = mean(CASchools_OOS_stand$testscore)
CASchools_OOS_stand$testscore = CASchools_OOS_stand$testscore - testscore_mean
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test))
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test))
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
pred_pca = predict(model_pca, newx = as.matrix(x_test))
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10)
plot(cv_ridge)
cv_ridge$lambda.min
# CV for lasso  regression
cv_lasso <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 1, nfolds = 10)
plot(cv_lasso)
cv_lasso$lambda.min
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10, type.measure = 'mse')
plot(cv_ridge)
cv_ridge$lambda.min
# CV for PCA
# Specify the range of possible numbers of components
num_components_range <- 1:100
# Create a data frame to store the results
results <- data.frame(num_components = num_components_range, RMSE = rep(NA, length(num_components_range)))
# Specify the cross-validation settings
ctrl <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Iterate over the range of possible numbers of components
for (i in num_components_range) {
# Perform PCA
pca_model <- prcomp(CA_x, scale. = TRUE)
# Extract principal components
x_train_pca <- predict(pca_model, newdata = CA_x)[, 1:i]
x_train_pca_df <- as.data.frame(x_train_pca)
colnames(x_train_pca_df) <- paste0("PC", 1:i)
# Fit a predictive model (e.g., linear regression) on the reduced-dimensional data using cross-validation
lm_model <- train(x = x_train_pca_df, y = CASchool_stand$testscore, method = "lm", trControl = ctrl)
# Store the RMSE from cross-validation
results$RMSE[i] <- min(lm_model$results$RMSE)
}
# Find the number of components with the lowest RMSE
optimal_num_components <- results$num_components[which.min(results$RMSE)]
# Print the optimal number of components
print(optimal_num_components)
# model using optimal lambdas
ridge_model <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha = 0,  lambda = cv_ridge$lambda.min, intercept = FALSE)
lasso_model  <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha =1,  lambda = cv_lasso$lambda.min, intercept = FALSE)
first_13_components <- pca_result$x[, 1:13]
model_pca = lm(CASchool_stand$testscore~first_13_components - 1)
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test), s =cv_ridge$lambda.min)
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test), s=cv_lasso$lambda.min)
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
x_test_pca = predict(pca_result, newdata = x_test)[, 1:23]
pred_pca = predict(model_pca, newx = x_test_pca)
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
install.packages("strucchange")
install.packages("strucchange")
install.packages("strucchange")
install.packages("strucchange",lib="C:\Users\zhouq")
install.packages("strucchange",lib="C:/Users/zhouq")
# install.packages("strucchange",lib="C:/Users/zhouq")
library(strucchange)
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths <- c(myPaths, "C:/Users/zhouq")
.libPaths(myPaths)
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths = c(myPaths, "C:/Users/zhouq")
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths <- .libPaths()
myPaths = c(myPaths, "C:/Users/zhouq")
.libPaths(myPaths)
install.packages("strucchange", lib="C:/Users/zhouq")
install.packages("dplR", lib="C:/Users/zhouq")
library(strucchange)
library(readr)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP')
library(readr)
quaterly_data_raw <- read_csv("quaterly_data_raw.csv",
col_types = cols(Date = col_date(format = "%m/%d/%Y")))
install.packages("vroom")
quaterly_data_raw <- read_csv("quaterly_data_raw.csv")
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP')
library(readr)
quaterly_data_raw <- read_csv("quaterly_data_raw.csv")
quaterly_data_raw <- read_csv("quaterly_data_raw.csv", show_col_types =  False)
quaterly_data_raw <- read_csv("quaterly_data_raw.csv", show_col_types =  FALSE)
View(quaterly_data_raw)
quaterly_data_raw$Date = format(quaterly_data_raw$Date, format = '%m/%d/%Y')
View(quaterly_data_raw)
quaterly_data_raw[2,1]-quaterly_data_raw[3,1]
quaterly_data_raw[3,1]
as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
quaterly_data_raw$Date = as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
summary(quaterly_data_raw)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP')
library(readr)
quaterly_data_raw <- read_csv("quaterly_data_raw.csv", show_col_types =  FALSE)
quaterly_data_raw$Date = as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
# SPECIFY FUNCTIONS FOR DATA TRANSFORMATION
D = function(x){c(NA,diff(x))}
DD = function(x){c(NA,NA,diff(diff(x)))}
L = function(x){c(log(x))}
DL = function(x){c(NA,diff(log(x)))}
DDL = function(x){c(NA,NA,diff(diff(log(x))))}
startDate = "1959-01-01"
endDate = "2019-12-31"
date_format = '%d/%m/%Y'
# quaterly_data_raw$Date = format(quaterly_data_raw$Date, format = date_format)
date_filtering = function(x,start_date, end_date, date_format){
x$Date = as.Date(x$Date,format = date_format)
x = x[x$Date >= start_date & x$Date <= end_date,]
return(x)
}
# transformation type is in first row
transform = function(x){
c2 = which(x[1,]==2)
c3 = which(x[1,]==3)
c4 = which(x[1,]==4)
c5 = which(x[1,]==5)
c6 = which(x[1,]==6)
x1 = x[-(1),] # remove first row
for (i in c2){
tt = D(x1[,i])
x1[,i] = tt}
for (i in c3){
tt = DD(x1[,i])
x1[,i] = tt }
for (i in c4){
tt = L(x1[,i])
x1[,i] = tt }
for (i in c5){
tt = DL(x1[,i])
x1[,i] = tt}
for (i in c6){
tt = DDL(x1[,i])
x1[,i] = tt }
x = x1}
x = date_filtering(quaterly_data_raw,
start_date =  startDate,
end_date = endDate,
date_format = date_format)
View(x)
data_transform = transform(x)
View(data_transform)
x
quaterly_data_raw <- read.csv("quaterly_data_raw.csv", show_col_types =  FALSE)
quaterly_data_raw <- read.csv("quaterly_data_raw.csv")
View(quaterly_data_raw)
quaterly_data_raw$Date = as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
summary(quaterly_data_raw)
quaterly_data_raw <- read.csv("quaterly_data_raw.csv")
quaterly_data_raw$Date = as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
summary(quaterly_data_raw)
# SPECIFY FUNCTIONS FOR DATA TRANSFORMATION
D = function(x){c(NA,diff(x))}
DD = function(x){c(NA,NA,diff(diff(x)))}
L = function(x){c(log(x))}
DL = function(x){c(NA,diff(log(x)))}
DDL = function(x){c(NA,NA,diff(diff(log(x))))}
startDate = "1959-01-01"
endDate = "2019-12-31"
date_format = '%d/%m/%Y'
# quaterly_data_raw$Date = format(quaterly_data_raw$Date, format = date_format)
date_filtering = function(x,start_date, end_date, date_format){
x$Date = as.Date(x$Date,format = date_format)
x = x[x$Date >= start_date & x$Date <= end_date,]
return(x)
}
# transformation type is in first row
transform = function(x){
c2 = which(x[1,]==2)
c3 = which(x[1,]==3)
c4 = which(x[1,]==4)
c5 = which(x[1,]==5)
c6 = which(x[1,]==6)
x1 = x[-(1),] # remove first row
for (i in c2){
tt = D(x1[,i])
x1[,i] = tt}
for (i in c3){
tt = DD(x1[,i])
x1[,i] = tt }
for (i in c4){
tt = L(x1[,i])
x1[,i] = tt }
for (i in c5){
tt = DL(x1[,i])
x1[,i] = tt}
for (i in c6){
tt = DDL(x1[,i])
x1[,i] = tt }
x = x1}
x = date_filtering(quaterly_data_raw,
start_date =  startDate,
end_date = endDate,
date_format = date_format)
View(x)
data_transform = transform(x)
View(data_transform)
x = date_filtering(quaterly_data_raw,
start_date =  startDate,
end_date = endDate,
date_format = date_format)
quaterly_data_raw <- read.csv("quaterly_data_raw.csv")
quaterly_data_raw$Date = as.Date(quaterly_data_raw$Date, format = '%m/%d/%Y')
x = date_filtering(quaterly_data_raw,
start_date =  startDate,
end_date = endDate,
date_format = date_format)
View(x)
View(x)
date_filtering = function(x,start_date, end_date, date_format){
transform_header = x[1,]
x$Date = as.Date(x$Date,format = date_format)
x = x[x$Date >= start_date & x$Date <= end_date,]
x[1,] = transform_header
return(x)
}
View(x)
x = date_filtering(quaterly_data_raw,
start_date =  startDate,
end_date = endDate,
date_format = date_format)
View(x)
data_transform = transform(x)
View(data_transform)
library(readr)
Data_transform <- read_csv("Data_transform.csv")
View(Data_transform)
write.csv(data_transform, "Data_Q_transformed.csv", row.names=FALSE)
result <- data.frame(Variable = character(), p_value = numeric(), Reject_H0 = character(), stringsAsFactors = FALSE)
for (col in colnames(data_transform)) {
test_result = adf.test(na.omit(data_transform[[col]]))
p_value = test_result$p.value
reject = ifelse(p_value < 0.05, "Yes", "No")
result = rbind(result, data.frame(Variable = col, p_value = p_value, Reject_H0 = reject))
}
# unit root test to check that all data are stationary
# Agumented Dicky-Fullter test
# H0: not stationary vs H1: stationary
library(lubridate)
library(tseries)
result <- data.frame(Variable = character(), p_value = numeric(), Reject_H0 = character(), stringsAsFactors = FALSE)
for (col in colnames(data_transform)) {
test_result = adf.test(na.omit(data_transform[[col]]))
p_value = test_result$p.value
reject = ifelse(p_value < 0.05, "Yes", "No")
result = rbind(result, data.frame(Variable = col, p_value = p_value, Reject_H0 = reject))
}
result
for (col in colnames(data_transform)) {
test_result = adf.test(na.omit(data_transform[[col]]))
p_value = test_result$p.value
reject = ifelse(p_value < 0.05, "Stationary", "Not Stationary")
result = rbind(result, data.frame(Variable = col, p_value = p_value, Reject_H0 = reject))
}
result
#output results as LATEX table
library(xtable)
result <- data.frame(Variable = character(), p_value = numeric(), Reject_H0 = character(), stringsAsFactors = FALSE)
result = rbind(result, data.frame(Variable = col, p_value = p_value, Stationary? = reject))
for (col in colnames(data_transform)) {
test_result = adf.test(na.omit(data_transform[[col]]))
p_value = test_result$p.value
reject = ifelse(p_value < 0.05, "Stationary", "Not Stationary")
result = rbind(result, data.frame(Variable = col, p_value = p_value, Stationary = reject))
}
use warnings()
warnings()
result
