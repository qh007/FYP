stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(stand_col))) {
CASchool_stand[stand_col[i]] = (CASchool_stand[stand_col[i]] - mean(CASchool_stand[[stand_col[i]]])) / sd(CASchool_stand[[stand_col[i]]]) }
# sample mean of TestScore
testscore_mean = mean(CASchool_stand$testscore)
testscore_mean
CASchool_stand$testscore = CASchool_stand$testscore - testscore_mean
# OLS regression results
CASchool_stand = CASchool_stand %>% select(-c("countyname","districtname","schoolname","zipcode"))
model <- lm(testscore ~. - 1, data = CASchool_stand)
summary(model)
# note that for lambda shd be 300/500
CA_x = CASchool_stand %>% select(-c(testscore))
ridge_model <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha = 0,  lambda = 300/500, intercept = FALSE, standardize = FALSE)
summary(ridge_model)
# print(coef(ridge_model))
# lambda shd be 1000/500
lasso_model  <- glmnet(x = as.matrix(CA_x), y = CASchool_stand$testscore, alpha =1,  lambda = 2, intercept = FALSE, standardize = FALSE)
print(lasso_model)
non_zero_counts <- apply(coef(lasso_model) != 0, 2, sum)
print(non_zero_counts)
pca_result <- prcomp(CA_x, scale. = TRUE)
variance_proportion <- pca_result$sdev^2 / sum(pca_result$sdev^2)
plot(variance_proportion, type = "b", xlab = "Principal Component", ylab = "Proportion of Variance Explained", main = "Scree Plot")
variance_proportion[1]*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:2])*100
sum(variance_proportion[1:15])*100
first_15_components <- pca_result$x[, 1:15]
print(head(first_15_components))
model_pca = lm(CASchool_stand$testscore~first_15_components - 1)
summary(model_pca)
CASchools_OOS = read_excel("CASchools_EE141_OutOfSample.xlsx")
# construct new dataset to include squares and interactions for predictors
all_col = colnames(CASchools_OOS)
x_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore","charter_s")]
# add squares
for (i in seq(1, length(x_col))) {CASchools_OOS[paste(x_col[i],'squared', sep = '_')] = CASchools_OOS[x_col[i]]^2}
# add interactions
x_col = append(x_col, "charter_s")
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
CASchools_OOS[paste(x_col[i],x_col[j], sep = '_')] = CASchools_OOS[x_col[i]]*CASchools_OOS[x_col[j]]} }}
# standarise predictors
CASchools_OOS_stand = CASchools_OOS
all_col = colnames(CASchools_OOS_stand)
stand_col = all_col[!all_col %in% c("countyname","districtname","schoolname","zipcode","testscore")]
for (i in seq(1, length(x_col))) {
CASchools_OOS_stand[stand_col[i]] = (CASchools_OOS_stand[stand_col[i]] - mean(CASchools_OOS_stand[[stand_col[i]]])) / sd(CASchools_OOS_stand[[stand_col[i]]]) }
# demean testscore
testscore_mean = mean(CASchools_OOS_stand$testscore)
CASchools_OOS_stand$testscore = CASchools_OOS_stand$testscore - testscore_mean
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test))
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test))
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
pred_pca = predict(model_pca, newx = as.matrix(x_test))
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10)
plot(cv_ridge)
cv_ridge$lambda.min
# CV for lasso  regression
cv_lasso <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 1, nfolds = 10)
plot(cv_lasso)
cv_lasso$lambda.min
# CV for ridge regression
cv_ridge <- cv.glmnet(x= as.matrix(CA_x), y=CASchool_stand$testscore, alpha = 0, nfolds = 10, type.measure = 'mse')
plot(cv_ridge)
cv_ridge$lambda.min
# CV for PCA
# Specify the range of possible numbers of components
num_components_range <- 1:100
# Create a data frame to store the results
results <- data.frame(num_components = num_components_range, RMSE = rep(NA, length(num_components_range)))
# Specify the cross-validation settings
ctrl <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation
# Iterate over the range of possible numbers of components
for (i in num_components_range) {
# Perform PCA
pca_model <- prcomp(CA_x, scale. = TRUE)
# Extract principal components
x_train_pca <- predict(pca_model, newdata = CA_x)[, 1:i]
x_train_pca_df <- as.data.frame(x_train_pca)
colnames(x_train_pca_df) <- paste0("PC", 1:i)
# Fit a predictive model (e.g., linear regression) on the reduced-dimensional data using cross-validation
lm_model <- train(x = x_train_pca_df, y = CASchool_stand$testscore, method = "lm", trControl = ctrl)
# Store the RMSE from cross-validation
results$RMSE[i] <- min(lm_model$results$RMSE)
}
# Find the number of components with the lowest RMSE
optimal_num_components <- results$num_components[which.min(results$RMSE)]
# Print the optimal number of components
print(optimal_num_components)
# model using optimal lambdas
ridge_model <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha = 0,  lambda = cv_ridge$lambda.min, intercept = FALSE)
lasso_model  <- glmnet(x = CA_x, y = CASchool_stand$testscore, alpha =1,  lambda = cv_lasso$lambda.min, intercept = FALSE)
first_13_components <- pca_result$x[, 1:13]
model_pca = lm(CASchool_stand$testscore~first_13_components - 1)
x_test = CASchools_OOS_stand%>% select(-c("countyname","districtname","schoolname","zipcode","testscore"))
# OLS
pred_OLS = predict(model, newx = as.matrix(x_test))
mse_ols = mean((CASchools_OOS_stand$testscore - pred_OLS)^2)
rmse_ols = sqrt(mse_ols)
print(rmse_ols)
# Ridge
pred_ridge = predict(ridge_model, newx = as.matrix(x_test), s =cv_ridge$lambda.min)
mse_ridge = mean((CASchools_OOS_stand$testscore - pred_ridge)^2)
rmse_ridge = sqrt(mse_ridge)
print(rmse_ridge)
# Lasso
pred_lasso = predict(lasso_model, newx = as.matrix(x_test), s=cv_lasso$lambda.min)
mse_lasso = mean((CASchools_OOS_stand$testscore - pred_lasso)^2)
rmse_lasso = sqrt(mse_lasso)
print(rmse_lasso)
# PCA
x_test_pca = predict(pca_result, newdata = x_test)[, 1:23]
pred_pca = predict(model_pca, newx = x_test_pca)
mse_pca = mean((CASchools_OOS_stand$testscore - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
install.packages("strucchange")
install.packages("strucchange")
install.packages("strucchange")
install.packages("strucchange",lib="C:\Users\zhouq")
install.packages("strucchange",lib="C:/Users/zhouq")
# install.packages("strucchange",lib="C:/Users/zhouq")
library(strucchange)
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths <- c(myPaths, "C:/Users/zhouq")
.libPaths(myPaths)
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths = c(myPaths, "C:/Users/zhouq")
# install.packages("strucchange",lib="C:/Users/zhouq")
myPaths <- .libPaths()
myPaths = c(myPaths, "C:/Users/zhouq")
.libPaths(myPaths)
install.packages("strucchange", lib="C:/Users/zhouq")
install.packages("dplR", lib="C:/Users/zhouq")
library(strucchange)
library(readr)
knitr::opts_chunk$set(echo = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/Problem Sets/Problem set 8')
library(dplyr)
library(readxl)
library(tidyr)
library(glmnet)
# load dataset
data = read_xlsx('us_macro_quarterly.xlsx')
View(data)
# load dataset
data = read_xlsx('us_macro_quarterly.xlsx')
data$freq = as.Date(data$freq, format = 'YYYY-MM-DD')
# select 1963 - 2017
data = data %>% select(freq)
View(data)
# load dataset
data = read_xlsx('us_macro_quarterly.xlsx')
data$freq = as.Date(data$freq, format = 'YYYY-MM-DD')
View(data)
# select 1963 - 2017
# data = data %>% select(freq)
# load dataset
data = read_xlsx('us_macro_quarterly.xlsx')
data$freq = as.Date(data$freq, format = 'YYYY-MM-DD')
View(data)
# select 1963 - 2017
data = data %>% filter(freq >= '1963-01-01')
bic = function(SSR){for (i in range (0, length(SSR))){
BIC = log(SSR[i]/372) + (p+1)*(log(372)/372)
print(BIC)}}
bic = function(SSR){for (i in range (0, length(SSR))){
BIC = log(SSR[i]/372) + (p+1)*(log(372)/372)
print(BIC)}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
bic = function(SSR){for (i in range (0, length(SSR))){
BIC = log(SSR[i]/372) + (i+1)*(log(372)/372)
print(BIC)}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
length(SSR)
SSR[1]
SSR[0]
bic = function(SSR){for (i in range (0, length(SSR)-1)){
BIC = log(SSR[i+1]/372) + (i+1)*(log(372)/372)
print(BIC)}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
bic = function(SSR){for (i in range (0, length(SSR)-1)){
BIC = log(SSR[i+1]/372) + (i+1)*(log(372)/372)
print(i)
print(BIC)
}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
bic = function(SSR){for (i in seq (0, length(SSR)-1)){
BIC = log(SSR[i+1]/372) + (i+1)*(log(372)/372)
print(i)
print(BIC)
}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
bic = function(SSR){for (i in seq (0, length(SSR)-1)){
BIC = log(SSR[i+1]/372) + (i+1)*(log(372)/372)
print(paste('lag',i)
print(BIC)
bic = function(SSR){for (i in seq (0, length(SSR)-1)){
BIC = log(SSR[i+1]/372) + (i+1)*(log(372)/372)
print(paste('lag',i))
print(BIC)
}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
bic(SSR)
aic =  function(SSR){for (i in seq (0, length(SSR)-1)){
AIC = log(SSR[i+1]/372) + (i+1)*(2/372)
print(paste('lag',i))
print(AIC)
}}
SSR = c(21045, 20043, 18870, 17838, 17344, 17337, 17306)
aic(SSR)
View(data)
x = diff(data$PCECTPI)
x
length(x)
knitr::opts_chunk$set(echo = TRUE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/Problem Sets/Problem set 8')
library(dplyr)
library(readxl)
library(tidyr)
library(glmnet)
data$infl = c(0, 400*(diff(ln(data$PCECTPI))))
data$infl = c(0, 400*(diff(log(data$PCECTPI))))
View(data)
# load dataset
data = read_xlsx('us_macro_quarterly.xlsx')
data$freq = as.Date(data$freq, format = 'YYYY-MM-DD')
View(data)
# select 1962 - 2017 (preserve the previous year for calculations)
data = data %>% filter(freq >= '1962-01-01')
data$infl = c(0, 400*(diff(log(data$PCECTPI))))
library(haven)
state <- read_dta("HE4043 - Finance & Development/3 - Lab exercises/T 08 political economy of finance/state.dta")
View(state)
summary(state)
library(dplyr)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(pls)
library(stargazer)
library(psych)
library(corrplot)
library(ggplot2)
house = read.csv('Data/housing_new.csv', header = T)
View(house)
train_size = as.integer(nrow(house)*0.8)
print(paste('Train size is:', train_size))
# Independent variables
house_x = house %>% select(-median_house_value)
x_train = house_x[1:train_size,]
x_test = house_x[(train_size+1):nrow(house_x) , ]
# Independent variable
y_train = house$median_house_value[1:train_size]
y_test = house$median_house_value[(train_size+1):nrow(house_x)]
sd(y_train)
sd(y_test)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
library(dplyr)
library(stargazer)
library(readr)
library(mltools)
library(data.table)
housing = read_csv("Data/housing.csv", col_types = cols(ocean_proximity = col_character()))
# convert ocean proximity to categorical
housing$ocean_proximity = factor(housing$ocean_proximity, levels = c("ISLAND", "NEAR BAY", "NEAR OCEAN", "<1H OCEAN", "INLAND"), labels = c(0, 1, 2, 3, 4))
# select predictors
all_col = colnames(housing)
x_col = all_col[!all_col %in% c("longitude","latitude","median_house_value","ocean_proximity")]
# add squares
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'square', sep = '_')] = housing[[x_col[i]]]^2}
# add cubes
for (i in seq(1, length(x_col))) {housing[paste(x_col[i],'cube', sep = '_')] = housing[[x_col[i]]]^3}
# total variables shd be 10 + 6 + 6 = 22
# split ocean proximity into different dummy variables (total have 22 -1 + 5 =26)
housing_new = one_hot(as.data.table(housing))
housing_new = housing_new%>% rename("ISLAND" = "ocean_proximity_0" ,
"NEAR_BAY" = "ocean_proximity_1",
"NEAR_OCEAN" = "ocean_proximity_2",
"X1H_OCEAN" = "ocean_proximity_3",
"INLAND" = "ocean_proximity_4")
# set INLAND as base case and dont add its interactions
x_col = append(x_col, c("ISLAND", "NEAR_BAY", "NEAR_OCEAN", "X1H_OCEAN"))
housing_new = data.frame(housing_new)
for (i in seq(1, length(x_col))) { for (j in seq(1, length(x_col))) {
if (i < j ) {
new_varname = paste(x_col[i],x_col[j], sep = '_')
housing_new[new_varname] = housing_new[[x_col[i]]]*housing_new[[x_col[j]]]} }}
# total columns = 71 columns
# remove all na values
housing_new = na.omit(housing_new)
View(housing_new)
# drop all interactions of ocean_proximity category
housing_new = housing_new%>% select(-c('ISLAND_NEAR_BAY', 'ISLAND_NEAR_OCEAN', 'ISLAND_X1H_OCEAN', 'NEAR_BAY_NEAR_OCEAN', 'NEAR_BAY_X1H_OCEAN', 'NEAR_OCEAN_X1H_OCEAN', 'INLAND'))
# shd have 65 columns in total
# standardize all predictors, stand_x = (x_i - mean)/sd(x)
all_col = colnames(housing_new)
stand_col = all_col[!all_col %in% c("longitude","latitude")]
for (i in seq(1, length(stand_col))) {
housing_new[stand_col[i]] = (housing_new[stand_col[i]] - mean(housing_new[[stand_col[i]]])) / sd(housing_new[[stand_col[i]]]) }
housing_post = housing_new %>% select(-c("longitude","latitude","median_house_value"))
means <- colMeans(housing_post)
sds <- apply(housing_post, 2, sd)
summary_post <- data.frame(mean = means, sd = sds)
summary_post
housing_post = housing_new %>% select(-c("longitude","latitude"))
means <- colMeans(housing_post)
sds <- apply(housing_post, 2, sd)
summary_post <- data.frame(mean = means, sd = sds)
summary_post
write.csv(housing_new,file="Data/housing_new.csv", row.names=FALSE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(pls)
library(stargazer)
library(psych)
library(corrplot)
library(ggplot2)
house = read.csv('Data/housing_new.csv', header = T)
View(house)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/HE4903- Advanced Econometrics/HE4903 Project')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(pls)
library(stargazer)
library(psych)
library(corrplot)
library(ggplot2)
house = read.csv('Data/housing_new.csv', header = T)
View(house)
train_size = as.integer(nrow(house)*0.8)
print(paste('Train size is:', train_size))
# Independent variables
house_x = house %>% select(-median_house_value)
x_train = house_x[1:train_size,]
x_test = house_x[(train_size+1):nrow(house_x) , ]
# Independent variable
y_train = house$median_house_value[1:train_size]
y_test = house$median_house_value[(train_size+1):nrow(house_x)]
pc = prcomp(x_train)
sum_pc = summary(pc)
# print(sum_pc, digits = 3)
# First 15 PC explains 96.88% of variation; first 23 Pc 99%
#calculate total variance explained by each principal component
var_explained = pc$sdev^2 / sum(pc$sdev^2)
#create scree plot
qplot(c(1:30), var_explained[1:30]) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot (First 30 Principal Components)") +
ylim(0, 0.5)
set.seed(2)
pcr_fit = pcr(median_house_value ~ ., data = house[1:train_size, ], scale = FALSE, validation = "CV")
# plot of CV error
validationplot(pcr_fit, val.type = "RMSEP")
summary(pcr_fit)
# lowest RMSE when pc = 53
pred_pca = predict(pcr.fit, x_test, ncomp = 53)
pred_pca = predict(pcr_fit, x_test, ncomp = 53)
# PCA
# = predict(model_pca, newx = as.matrix(x_test))
mse_pca = mean((y_test - pred_pca)^2)
rmse_pca = sqrt(mse_pca)
print(rmse_pca)
View(house)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP/Data')
gc()
gc()
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP/Data')
library(lubridate)
library(tseries)
# Load Data
data = read.csv("UK.csv")
colnames(data)[colnames(data) == "Title"] <- "Date"
data = na.omit(data)
D = function(x){c(NA,diff(x))}
DD = function(x){c(NA,NA,diff(diff(x)))}
L = function(x){c(log(x))}
DL = function(x){c(NA,diff(log(x)))}
DDL = function(x){c(NA,NA,diff(diff(log(x))))}
plot(data$CPI)
plot(DDL(data$CPI)
)
adf.test(na.omit(data$CPI))
adf.test(na.omit(DDL(data$CPI)))
plot(data$HHC)
plot(D(data$HHC))
plot(DL(data$HHC))
adf.test(na.omit(DL(data$HHC)))
adf.test(na.omit(DDL(data$HHC)))
# transform GDP Data
data$GDP = DL(data$GDP)
data$CPI = DDL(data$CPI)
data$UnE = DDL(data$UnE)
data$Ex = D(data$Ex)
data$HHC = DDL(data$HHC)
adf.test(na.omit(data$GDP))
adf.test(na.omit(data$CPI))
adf.test(na.omit(data$UnE))
adf.test(na.omit(data$Ex))
adf.test(na.omit(DDL(data$HHC)))
data$HHC = DDL(data$HHC)
# Load Data
data = read.csv("UK.csv")
colnames(data)[colnames(data) == "Title"] <- "Date"
data = na.omit(data)
D = function(x){c(NA,diff(x))}
DD = function(x){c(NA,NA,diff(diff(x)))}
L = function(x){c(log(x))}
DL = function(x){c(NA,diff(log(x)))}
DDL = function(x){c(NA,NA,diff(diff(log(x))))}
# transform GDP Data
data$GDP = DL(data$GDP)
data$CPI = DDL(data$CPI)
data$UnE = DDL(data$UnE)
data$Ex = D(data$Ex)
data$HHC = DDL(data$HHC)
adf.test(na.omit(data$GDP))
adf.test(na.omit(data$CPI))
adf.test(na.omit(data$UnE))
adf.test(na.omit(data$Ex))
adf.test(na.omit(data$HHC))
write.csv(data, "UK_transformed.csv", row.names=FALSE)
setwd('C:/Users/zhouq/OneDrive - Nanyang Technological University/FYP/Codes/FYP/Data')
library(lubridate)
library(tseries)
# Load Data
data = read.csv("UK.csv")
colnames(data)[colnames(data) == "Title"] <- "Date"
data = na.omit(data)
D = function(x){c(NA,diff(x))}
DD = function(x){c(NA,NA,diff(diff(x)))}
L = function(x){c(log(x))}
DL = function(x){c(NA,diff(log(x)))}
DDL = function(x){c(NA,NA,diff(diff(log(x))))}
# transform GDP Data
data$GDP = DL(data$GDP)
data$CPI = DDL(data$CPI)
data$UnE = DDL(data$UnE)
data$Ex = D(data$Ex)
data$HHC = DDL(data$HHC)
adf.test(na.omit(data$RPIF))
adf.test(na.omit(DDL(data$RPIF)))
adf.test(na.omit(data$CPA))
adf.test(na.omit(DDL(data$CPA)))
adf.test(na.omit(DL(data$CPA)))
transform GDP Data
# Load Data
data = read.csv("UK.csv")
colnames(data)[colnames(data) == "Title"] <- "Date"
# transform GDP Data
data$GDP = DL(data$GDP)
data$CPI = DDL(data$CPI)
data$UnE = DDL(data$UnE)
data$Ex = D(data$Ex)
data$HHC = DDL(data$HHC)
data$RPIF = DDL(data$RPIF)
data$CPA = DL(data$CPA)
# Load Data
data = read.csv("UK.csv")
# Load Data
data = read.csv("UK.csv")
colnames(data)[colnames(data) == "Title"] <- "Date"
data = na.omit(data)
# transform GDP Data
data$GDP = DL(data$GDP)
data$CPI = DDL(data$CPI)
data$UnE = DDL(data$UnE)
data$Ex = D(data$Ex)
data$HHC = DDL(data$HHC)
data$RPIF = DDL(data$RPIF)
data$CPA = DL(data$CPA)
adf.test(na.omit(data$GDP))
adf.test(na.omit(data$CPI))
adf.test(na.omit(data$UnE))
adf.test(na.omit(data$Ex))
adf.test(na.omit(data$HHC))
adf.test(na.omit(data$RPIF))
adf.test(na.omit(data$CPA))
write.csv(data, "UK_transformed.csv", row.names=FALSE)
